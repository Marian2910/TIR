{\color{gray}\hrule}
\begin{center}
\section{Trend reconstruction}
\textbf{Chronological evolution of the trend}
\bigskip
\end{center}
{\color{gray}\hrule}
\begin{multicols}{2}
\subsection{Early approaches}
Early approaches date back to 2010, with Docker being released in 2013. In 2006, the cost of electricity consumed by IT infrastructures in the US was estimated at 4.5 billion dollars and was projected to double by 2011 \cite{beloglazov_energy_2010}.

In the initial attempts described in\cite{beloglazov_energy_2010}, a simple Bin Packing variation with dynamic voltage and frequency scaling (DVFS) enabled was used to address this problem. The results, obtained using the CloudSim environment, showed an energy savings gain of 83\% compared to no policy, with an SLA violation of 1.1\%. This marked the beginning of research in this area.

By 2015, researchers shifted toward predictive modeling to overcome the limitations of static thresholds. In \cite{dabbagh_energy-efficient_2015},
a Wiener filter-based predictor was introduced to estimate cluster workloads, 
combined with Best Fit Decreasing for physical machine allocation.
This approach improved energy efficiency by up to 33\% compared to heuristic methods.
Shortly after, in 2017, further improvements were made to predictive models \cite{bui_energy_2017}.
This work extended previous research by incorporating Gaussian Process Regression (GPR) for non-stationary workload prediction,
accelerated using the Fast Fourier Transform to mitigate GPR's computational complexity.
Additionally, a convex optimization-based migration strategy was employed.
These enhancements resulted in a 35\% energy efficiency improvement over heuristic methods, with only a 15\% latency tradeoff.

\subsection{From Virtual Machines to Containers}

As cloud infrastructures evolve, the shift from VM-based to container-based deployments has become central to achieving energy efficiency and maintaining SLA compliance. Containers offer a lightweight alternative to VMs, reducing overhead and enabling more flexible resource allocation\cite{alahmad_availability-aware_2018}. Early container-based systems focused on reducing active VM counts through migration techniques—employing modular watchdogs and Pearson correlation checks to optimize resource usage\cite{alahmad_availability-aware_2018}.

Subsequent research advanced container scheduling by using flow network models to frame the allocation problem as a minimum cost flow problem. This approach effectively balances multi-resource constraints (e.g., CPU and memory) and has proven scalable in simulations involving up to 5000 machines\cite{hu_concurrent_2020}. 

More recent hybrid methods, which combine evolved rules from genetic programming with human-designed heuristics, address the two-level allocation challenge (assigning containers to VMs and VMs to physical machines). These techniques not only enhance energy efficiency but also improve fault tolerance by incorporating reliability metrics such as Mean Time To Failure (MTTF) and Mean Time To Repair (MTTR)\cite{tan_hybrid_2019, alahmad_availability-aware_2018}. Complementary studies, such as those by Kumar et al.\cite{kumar_renewable_2019} and Shi et al.\cite{shi_energy-aware_2018}, further demonstrate that container-based approaches can significantly reduce energy consumption and improve overall system performance.

This evolution—from basic migration techniques to advanced, hybrid scheduling—forms a critical foundation for the energy optimization and resilience strategies.


\end{multicols}


